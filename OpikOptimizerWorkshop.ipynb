{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/next-drought/ai-evaluation-workshops/blob/main/OpikOptimizerWorkshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Smarter Prompting, Faster: Introducing Opik's Agent Optimizers\n",
        "\n",
        "Doug Blank, Phd\n",
        "\n",
        "* Slides are available at: [bit.ly/opik-optimizer-dsblank-slides](https://bit.ly/opik-optimizer-dsblank-slides)\n",
        "* This notebook is available at: [bit.ly/opik-optimizer-dsblank](https://bit.ly/opik-optimizer-dsblank)\n",
        "\n",
        "You will need:\n",
        "1. A Google account, for running a Colab Notebook  - [google.com](https://google.com)\n",
        "2. A Comet account, for seeing Opik visualizations (free!) - [comet.com](https://comet.com)\n",
        "3. An OpenAI account, for using an LLM\n",
        "[platform.openai.com/settings/organization/api-keys](https://platform.openai.com/settings/organization/api-keys)\n"
      ],
      "metadata": {
        "id": "NTaMBjChz7m5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "This pip-install takes about a minute."
      ],
      "metadata": {
        "id": "yM1lU0dBBnJs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Tx6HwuU1rB4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install opik-optimizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opik_optimizer\n",
        "opik_optimizer.__version__"
      ],
      "metadata": {
        "id": "pVgkLU3v2KD8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3ae1018-1176-4720-da6b-b550b70122c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.7.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opik\n",
        "\n",
        "# Configure Opik\n",
        "opik.configure()"
      ],
      "metadata": {
        "id": "H0DNm-un_0Np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80499ba-09ff-42c6-8c94-97a098f89476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Your Opik API key is available in your account settings, can be found at https://www.comet.com/api/my/settings/ for Opik cloud\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your Opik API key:··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: The API key provided is not valid on https://www.comet.com/. Please try again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "vN72mHQy_7Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save time (and money) durring this demonstration, we have capture the results of a previous run of all of these LLM interactions. Our goal is to make this not cost any money. However, we of course can guarantee that. **Use at your own risk**!\n",
        "\n",
        "To capture the perviously cached results:"
      ],
      "metadata": {
        "id": "lPe_J8388M4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik_optimizer.demo.cache import get_litellm_cache\n",
        "\n",
        "get_litellm_cache(\"opik-workshop\")"
      ],
      "metadata": {
        "id": "1ru08pDwh3PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Dataset\n",
        "\n",
        "In these set of experiments, we are going to use the **HotPotQA** dataset. This dataset was designed to be difficult for regular LLMs to handle. This dataset is called a \"**multi-hop**\" dataset because answering the questions involves multiple reasoning steps and multiple tool calls, where the LLM needs to infer relationships, combine information, or draw conclusions based on the combined context.\n",
        "\n",
        "Example:\n",
        "\n",
        "> \"What are the capitals of the states that border California?\"\n",
        "\n",
        "You'd need to find which states border California, and then lookup each state's capital.\n",
        "\n",
        "The dataset has about 113,000 such crowd-sourced questions that are constructed to require the introductory paragraphs of two Wikipedia articles to answer.\n",
        "\n",
        "[1] The name \"HotPot\" comes from the restaurant where the authors came up with the idea of the dataset."
      ],
      "metadata": {
        "id": "zVK_KLEXbyqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik_optimizer.demo import get_or_create_dataset\n",
        "\n",
        "opik_dataset = get_or_create_dataset(\"hotpot-300\")"
      ],
      "metadata": {
        "id": "4gCj_2M3A37D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at some dataset items:"
      ],
      "metadata": {
        "id": "YvqslJnA9RMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = opik_dataset.get_items()\n",
        "rows[0]"
      ],
      "metadata": {
        "id": "KGZoscWiBInJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows[1]"
      ],
      "metadata": {
        "id": "8MQC4TllBKgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Opik Project\n",
        "\n",
        "All LLM traces in Opik are saved in a \"project\". We'll put them all in the following project name:"
      ],
      "metadata": {
        "id": "feVF8adydeyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_name = \"optimize-workshop-2025\""
      ],
      "metadata": {
        "id": "gTTZ5apsPgUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Metric\n",
        "\n",
        "Choosing a good metric for optimization is tricky. For these examples, we'll pick one that will allow us to show improvement, and also provide a gradient of scores. In general though, this metric isn't the best for optimization runs.\n",
        "\n",
        "We'll use \"Edit Distance\" AKA \"Levenshtein Distance\":"
      ],
      "metadata": {
        "id": "VEEHnyrcdjVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik.evaluation.metrics import LevenshteinRatio\n",
        "metric = LevenshteinRatio(project_name=project_name)"
      ],
      "metadata": {
        "id": "T3BKOXxPOpkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metric takes two things: the output of the LLM and the reference (correct answer)."
      ],
      "metadata": {
        "id": "BF9czV9Ue02R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric.score(\"Hello\", \"Hello\")"
      ],
      "metadata": {
        "id": "_hMQuJsSexkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric.score(\"Hello!\", \"Hello\")"
      ],
      "metadata": {
        "id": "hNleaL5ZPLaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The edit distance between \"Hello!\" and \"Hello\" is 1. Here is how the .91 is computed:"
      ],
      "metadata": {
        "id": "2eI1S-Q_-SwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edit_distance = 1\n",
        "\n",
        "1 - edit_distance / (len(\"Hello1\") + len(\"Hello\"))\n"
      ],
      "metadata": {
        "id": "7YEltgab4jPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more information see: [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance)"
      ],
      "metadata": {
        "id": "p3gIizQt47AP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuation\n",
        "\n",
        "To create the necesary configurations for using an Opik Optimizer, you'll need three things:\n",
        "\n",
        "1. An initial prompt\n",
        "2. A MetricConfig\n",
        "3. A TaskConfig\n",
        "\n",
        "We're going to start with a pretty bad prompt... so we can optimize it!"
      ],
      "metadata": {
        "id": "JarFSV9jb7jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_prompt = \"Provide an answer to the question\""
      ],
      "metadata": {
        "id": "xxDUfpvgPct7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The the two configurations:"
      ],
      "metadata": {
        "id": "Oia79J_e-vSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik_optimizer import (\n",
        "    MetricConfig,\n",
        "    TaskConfig,\n",
        "    from_llm_response_text,\n",
        "    from_dataset_field,\n",
        ")\n",
        "\n",
        "metric_config = MetricConfig(\n",
        "    metric=LevenshteinRatio(project_name=project_name),\n",
        "    inputs={\n",
        "        \"output\": from_llm_response_text(),\n",
        "        \"reference\": from_dataset_field(name=\"answer\"),\n",
        "    },\n",
        ")\n",
        "\n",
        "task_config = TaskConfig(\n",
        "    instruction_prompt=initial_prompt,\n",
        "    input_dataset_fields=[\"question\"],\n",
        "    output_dataset_field=\"answer\",\n",
        "    use_chat_prompt=True,\n",
        ")"
      ],
      "metadata": {
        "id": "ET_umnWdPmjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the MetricConfig is composed of our chosen metric. In addition, we need to know what the inputs will be. The inputs here are actually the outputs from the LLM.\n",
        "\n",
        "We need two inputs for the metric:\n",
        "1. The output produced by the LLM (uses a special name)\n",
        "2. The correct answer (provided by the database item \"answer\")\n",
        "\n",
        "The TaskConfig defines how to process a prompt. We need the initial prompt, and the inputs and outputs of the dataset.\n",
        "\n",
        "In this case, we will use the chat_prompt format as our result."
      ],
      "metadata": {
        "id": "2aDp9vjw5hIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FewShotBayesianOptimizer\n",
        "\n",
        "The FewShotBayesianOptimizer name indicates two things:\n",
        "\n",
        "1. It will produce Chat Prompts, or FewShot examples as described in the slides.\n",
        "2. Secondly, it describes how it searches for the best set of these FewShot examples.\n",
        "\n",
        "To use this optimizer, we import it and create an instance, passing in the project name and model parameters:"
      ],
      "metadata": {
        "id": "VBNAMIglcVIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik_optimizer import (\n",
        "    FewShotBayesianOptimizer,\n",
        ")\n",
        "\n",
        "optimizer = FewShotBayesianOptimizer(\n",
        "    project_name=project_name,\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=5000,\n",
        ")"
      ],
      "metadata": {
        "id": "EbaysjG1PRNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline\n",
        "\n",
        "Before we optimize this prompt (\"Provide an answer to the question\") let's see what the bare prompt does by itself on the dataset:"
      ],
      "metadata": {
        "id": "zuZkkZqGWg4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = optimizer.evaluate_prompt(\n",
        "    dataset=opik_dataset,\n",
        "    metric_config=metric_config,\n",
        "    task_config=task_config,\n",
        "    prompt=initial_prompt,\n",
        "    n_samples=100,\n",
        ")\n",
        "score"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fYEhqP2WP0B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It scored about 16% correct. [I say \"percent correct\" but because we are using edit distance, that isn't quite accurate. But we can think of it this way.]\n",
        "\n",
        "Ok, let's optimize that prompt!"
      ],
      "metadata": {
        "id": "UtFggdB67tmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = optimizer.optimize_prompt(\n",
        "    opik_dataset,\n",
        "    metric_config,\n",
        "    task_config,\n",
        "    n_trials=3,\n",
        "    n_samples=50\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3Q3ENgSARX2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1.display()"
      ],
      "metadata": {
        "id": "nJVmhi5Gcnkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What did we find? The result is a series of messages:"
      ],
      "metadata": {
        "id": "3_ppCUe6AGH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result1.details[\"chat_messages\"]"
      ],
      "metadata": {
        "id": "o1nDwurG_2qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll see how we can use those in a few minutes."
      ],
      "metadata": {
        "id": "jwVDZcN1ALzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MetaPromptOptimizer\n",
        "\n",
        "The MetaPromptOptimizer uses a clever idea: have the LLM generate better prompts!\n",
        "\n",
        "Here is the internal system meta-prompt to have the LLM generate better prompts.\n",
        "\n",
        "```text\n",
        "You are an expert prompt engineer. Your task is to improve prompts for any type of task.\n",
        "\n",
        "Focus on making the prompt more effective by:\n",
        "\n",
        "1. Being clear and specific about what is expected\n",
        "2. Providing necessary context and constraints\n",
        "3. Guiding the model to produce the desired output format\n",
        "4. Removing ambiguity and unnecessary elements\n",
        "5. Maintaining conciseness while being complete\n",
        "\n",
        "Return a JSON array of prompts with the following structure:\n",
        "{\n",
        "    \"prompts\": [\n",
        "        {\n",
        "            \"prompt\": \"the improved prompt text\",\n",
        "            \"improvement_focus\": \"what aspect this prompt improves\",\n",
        "            \"reasoning\": \"why this improvement should help\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "This can work quite well on simpler datasets. It doesn't do so well on HotPot as we will see.\n",
        "\n",
        "The MetaPromptOptimizer will try a number of rounds to try to find the best prompt."
      ],
      "metadata": {
        "id": "PjAeG6L8cdYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik_optimizer import (\n",
        "    MetaPromptOptimizer,\n",
        ")\n",
        "\n",
        "optimizer = MetaPromptOptimizer(\n",
        "    project_name=project_name,\n",
        "    model=\"openai/gpt-4o-mini\",  # Using gpt-4o-mini for evaluation for speed\n",
        "    max_rounds=1,  # Number of optimization rounds\n",
        "    num_prompts_per_round=2,  # Number of prompts to generate per round\n",
        "    improvement_threshold=0.01,  # Minimum improvement required to continue\n",
        "    temperature=0.1,  # Lower temperature for more focused responses\n",
        "    max_completion_tokens=5000,  # Maximum tokens for model completion\n",
        "    num_threads=1,  # Number of threads for parallel evaluation\n",
        "    subsample_size=20,  # Fixed subsample size\n",
        ")\n"
      ],
      "metadata": {
        "id": "2brgT9EGXDRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We won't do too many rounds, as this is an impossible problem without tools."
      ],
      "metadata": {
        "id": "-RwrEfDCAZng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = optimizer.optimize_prompt(\n",
        "    dataset=opik_dataset,\n",
        "    metric_config=metric_config,\n",
        "    task_config=task_config,\n",
        "    auto_continue=False,\n",
        "    n_samples=20,  # Explicitly set\n",
        "    use_subsample=True,  # Force using subsample for evaluation rounds\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "S2JmhR9kavmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result2.display()"
      ],
      "metadata": {
        "id": "XkHo9GlWckWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MiproOptimizer\n",
        "\n",
        "MIPRO (Multi-Iteration Prompt Optimization) is an optimizer algorithm that refines both prompts and few-shot examples in a multi-stage LLM program. It works by generating, evaluating, and refining prompts to improve language model performance. MIPRO is a more advanced method than simply \"prompt hacking,\" offering real optimization of LLM workflows.\n",
        "\n",
        "This sophisticated method optimizes both instructions and examples together. Using Bayesian optimization (like the FewShotBayesianOptimizer), it finds the best combinations of both elements. Through multiple testing rounds, it creates an optimized prompt that pairs effective instructions with relevant examples.\n",
        "\n",
        "For thi first optimization, we aren't going to give it any tools to work with. Let's see how it works:"
      ],
      "metadata": {
        "id": "z9DhnkNkkY2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik_optimizer import MiproOptimizer\n",
        "\n",
        "optimizer = MiproOptimizer(\n",
        "    model=\"openai/gpt-4o-mini\",  # LiteLLM or OpenAI name\n",
        "    project_name=project_name,\n",
        "    temperature=0.1,\n",
        "    num_threads=16,\n",
        ")"
      ],
      "metadata": {
        "id": "P5v7cOoBS-np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that we are still starting with the initial prompt:"
      ],
      "metadata": {
        "id": "lM3HmAKaES5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_prompt"
      ],
      "metadata": {
        "id": "eQw3J9jaOn4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result3 = optimizer.optimize_prompt(\n",
        "    dataset=opik_dataset,\n",
        "    metric_config=metric_config,\n",
        "    task_config=task_config,\n",
        "    n_samples=50,\n",
        ")"
      ],
      "metadata": {
        "id": "-yTxOdJbUOX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result3.display()"
      ],
      "metadata": {
        "id": "qB57A11JkSAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result3.demonstrations"
      ],
      "metadata": {
        "id": "MlUMj1syDYEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent with Tools\n",
        "\n",
        "Now we'll try with tools. This will allow multi-prompt optimization."
      ],
      "metadata": {
        "id": "1finqE4pmOND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need a tool. We'll use this one from DSPy:"
      ],
      "metadata": {
        "id": "pnojJsGMEocF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools:\n",
        "import dspy\n",
        "\n",
        "def search_wikipedia(query: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    This agent is used to search wikipedia. It can retrieve additional details\n",
        "    about a topic.\n",
        "    \"\"\"\n",
        "    results = dspy.ColBERTv2(url=\"http://20.102.90.50:2017/wiki17_abstracts\")(\n",
        "        query, k=3\n",
        "    )\n",
        "    return [x[\"text\"] for x in results]"
      ],
      "metadata": {
        "id": "0JoJKDAITusm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out on a subject:"
      ],
      "metadata": {
        "id": "u5v4pbcJEvEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_wikipedia(\"Developmental Robotics\")"
      ],
      "metadata": {
        "id": "kfuWKvEbT1BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And it is easy to add the tools to the config. Let's go!"
      ],
      "metadata": {
        "id": "RYuUA68JEzd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task_config.tools = [search_wikipedia]\n",
        "\n",
        "result4 = optimizer.optimize_prompt(\n",
        "    dataset=opik_dataset,\n",
        "    metric_config=metric_config,\n",
        "    task_config=task_config,\n",
        "    n_samples=50,\n",
        ")"
      ],
      "metadata": {
        "id": "puUovlhHkFOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result4.display()"
      ],
      "metadata": {
        "id": "ymJPBGJE4AOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result4.demonstrations"
      ],
      "metadata": {
        "id": "a2JE4hd7E9oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Optimized Prompts\n",
        "\n",
        "Recall:\n",
        "\n",
        "1. result1 - FewShotBayesianOptimizer\n",
        "2. result2 - MetaPromptOptimizer\n",
        "3. result3 - MiproOptimizer (no tools)\n",
        "4. result4 - MiproOptimizer (with search_wikipedia)\n",
        "\n",
        "How can we use the optimized results?\n",
        "\n",
        "For the first one, recall that the fewshot examples are here:"
      ],
      "metadata": {
        "id": "J7bVb_Pp9TUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result1.details[\"chat_messages\"]"
      ],
      "metadata": {
        "id": "B2wAB29G5V7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, once we have those we can do the following:"
      ],
      "metadata": {
        "id": "6uWZEjWTFcpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm.integrations.opik.opik import OpikLogger\n",
        "import litellm\n",
        "opik_logger = OpikLogger()\n",
        "litellm.callbacks = [opik_logger]\n",
        "\n",
        "def query(question, chat_messages):\n",
        "    messages = chat_messages[:-1] # Cut off the last one\n",
        "    # replace it with question in proper format:\n",
        "    messages.append({'role': 'user', 'content': '{\"question\": \"%s\"}\"}' % question})\n",
        "\n",
        "    response = litellm.completion(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0.1,\n",
        "        max_tokens=5000,\n",
        "        messages=messages,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "-wOD7oca9gjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query(\"When was David Chalmers born?\", result1.details[\"chat_messages\"])"
      ],
      "metadata": {
        "id": "2nu5Tqdl9wa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query(\"What weighs more: a baby elephant or an SUV?\", result1.details[\"chat_messages\"])"
      ],
      "metadata": {
        "id": "xI3mOXQ4_btf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If it says \"elephant\" that is not correct!\n",
        "\n",
        "Let's try that same question with a tool:"
      ],
      "metadata": {
        "id": "2x0JkiYhFsQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = result4.details[\"program\"](question=\"What weighs more: a baby elephant or an SUV?\")\n",
        "result.answer"
      ],
      "metadata": {
        "id": "77JY1iG3ANCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well done optimizer!\n",
        "\n",
        "We'll now head back to the slides to summarize the workshop."
      ],
      "metadata": {
        "id": "1Zw5UhcCF3lm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "\n",
        "1. [Opik Optimizer Workshop Slides](https://bit.ly/opik-optimizer-dsblank-slides)"
      ],
      "metadata": {
        "id": "uSDJ1bFx51kd"
      }
    }
  ]
}